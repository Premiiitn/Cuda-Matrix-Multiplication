{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Ym6wfSmj1X",
        "outputId": "333be2e4-4596-44c6-dc41-4b2fd410b70d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep  6 17:47:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeKyDTOtmC1x",
        "outputId": "0f47f695-c77f-4759-e749-7388ce5e2cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting inner_product_with_testbench.cu\n"
          ]
        }
      ],
      "source": [
        "# Write the modified CUDA code to a file\n",
        "%%writefile inner_product_with_testbench.cu\n",
        "\n",
        "// Import required header files\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel function for inner-product-based matrix multiplication\n",
        "__global__ void innerProductKernel(float *A, float *B, float *C, int N) {\n",
        "    // Calculate row and column indices for the thread\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Declare a variable to accumulate the sum\n",
        "    float sum = 0;\n",
        "\n",
        "    // Check if the thread's indices are within the matrix dimensions\n",
        "    if(row < N && col < N) {\n",
        "        // Compute the dot product for the i-th row of A and the j-th column of B\n",
        "        for(int k = 0; k < N; ++k) {\n",
        "            sum += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        // Store the sum in the corresponding element of matrix C\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to test the inner product implementation\n",
        "void testInnerProduct(float *A, float *B, float *C, int N) {\n",
        "    // Declare device pointers for matrices A, B, and C\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Error handling for CUDA operations\n",
        "    cudaError_t err;\n",
        "\n",
        "    // Allocate memory on the GPU for matrices A, B, and C\n",
        "    err = cudaMalloc((void **)&d_A, N * N * sizeof(float));\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA malloc A: %s\\n\", cudaGetErrorString(err));\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    err = cudaMalloc((void **)&d_B, N * N * sizeof(float));\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA malloc B: %s\\n\", cudaGetErrorString(err));\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    err = cudaMalloc((void **)&d_C, N * N * sizeof(float));\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA malloc C: %s\\n\", cudaGetErrorString(err));\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    // Copy the data for matrices A and B from host to device\n",
        "    cudaMemcpy(d_A, A, N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define the number of threads per block and the number of blocks per grid\n",
        "    dim3 threadsPerBlock(2, 2);\n",
        "    dim3 blocksPerGrid(N / threadsPerBlock.x, N / threadsPerBlock.y);\n",
        "\n",
        "    // Launch the kernel\n",
        "    innerProductKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Check for errors in kernel launch\n",
        "    err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    // Copy the resultant matrix C from device to host\n",
        "    cudaMemcpy(C, d_C, N * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free the allocated device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "}\n",
        "\n",
        "// Main function\n",
        "int main() {\n",
        "    // Define the dimension of the matrices\n",
        "    int N = 4;\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    float A[N * N] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};\n",
        "    float B[N * N] = {16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1};\n",
        "\n",
        "    // Declare the result matrix C\n",
        "    float C[N * N];\n",
        "\n",
        "    // Run the test function\n",
        "    testInnerProduct(A, B, C, N);\n",
        "\n",
        "    // Print the resultant matrix C\n",
        "    printf(\"Matrix C: \\n\");\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            printf(\"%f \", C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Exit the program\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the CUDA code with nvcc (NVIDIA CUDA Compiler)\n",
        "!nvcc inner_product_with_testbench.cu -o inner_product_with_testbench\n",
        "\n",
        "# Run the compiled CUDA program\n",
        "!./inner_product_with_testbench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9XXSB-cnDUb",
        "outputId": "72e330de-032b-4ea9-a0e8-5b96fb0a79a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix C: \n",
            "80.000000 70.000000 60.000000 50.000000 \n",
            "240.000000 214.000000 188.000000 162.000000 \n",
            "400.000000 358.000000 316.000000 274.000000 \n",
            "560.000000 502.000000 444.000000 386.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Matrix Multiplication with Inner Product Approach\n",
        "\n",
        "This notebook demonstrates how to perform matrix multiplication using the inner product approach on a GPU using CUDA. The code consists of two major parts:\n",
        "\n",
        "### Kernel Function (`innerProductKernel`)\n",
        "\n",
        "The `innerProductKernel` function is the GPU kernel that carries out the matrix multiplication. Each thread calculates one element of the resultant matrix \\( C \\) by taking the dot product of one row of matrix \\( A \\) with one column of matrix \\( B \\).\n",
        "\n",
        "- `__global__ void innerProductKernel(float *A, float *B, float *C, int N)`: This is the CUDA kernel function where the actual multiplication takes place.\n",
        "\n",
        "    - `float *A, *B, *C`: Pointers to matrices \\( A \\), \\( B \\), and \\( C \\) stored in the device memory.\n",
        "  \n",
        "    - `int N`: Dimension of the square matrices \\( A \\), \\( B \\), and \\( C \\).\n",
        "    \n",
        "    - `row` and `col`: Calculated to determine the row and column index for each thread.\n",
        "    \n",
        "    - `sum`: Variable to store the intermediate sum while calculating each element \\( C_{ij} \\).\n",
        "  \n",
        "### Test Function (`testInnerProduct`)\n",
        "\n",
        "The `testInnerProduct` function serves as a testbench. It allocates device memory, copies the input matrices from host to device, launches the kernel, and then copies the resultant matrix back to the host.\n",
        "\n",
        "- `void testInnerProduct(float *A, float *B, float *C, int N)`: Function that prepares the data, calls the CUDA kernel, and retrieves the data.\n",
        "\n",
        "    - `float *d_A, *d_B, *d_C`: Device pointers for matrices \\( A \\), \\( B \\), and \\( C \\).\n",
        "  \n",
        "    - `cudaMalloc`: Allocates memory on the device.\n",
        "    \n",
        "    - `cudaMemcpy`: Transfers data between host and device.\n",
        "    \n",
        "    - `innerProductKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N)`: Kernel invocation.\n",
        "    \n",
        "    - `cudaFree`: Frees the allocated device memory.\n",
        "\n",
        "### Compilation and Execution\n",
        "\n",
        "The code is compiled using the NVIDIA CUDA Compiler (`nvcc`) and executed on the GPU. The resultant matrix \\( C \\) is then printed on the console.\n",
        "\n",
        "To run this code:\n",
        "\n",
        "1. Use the `%%writefile` magic command to write the CUDA code into a `.cu` file.\n",
        "2. Use `!nvcc` to compile the code.\n",
        "3. Run the compiled executable with `!./inner_product_with_testbench`.\n",
        "\n",
        "This approach utilizes the parallel processing capabilities of a GPU to accelerate matrix multiplication."
      ],
      "metadata": {
        "id": "QDU55yhJytZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the CUDA code to a file\n",
        "%%writefile outer_product_with_testbench.cu\n",
        "\n",
        "// Import required header files\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel function for outer-product-based matrix multiplication\n",
        "__global__ void outerProductKernel(float *A, float *B, float *C, int N) {\n",
        "    // Calculate row and column indices for the thread\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Declare a variable to accumulate the sum\n",
        "    float sum = 0;\n",
        "\n",
        "    // Check if the thread's indices are within the matrix dimensions\n",
        "    if(row < N && col < N) {\n",
        "        // Compute the dot product for the i-th row of A and the j-th column of B\n",
        "        for(int k = 0; k < N; ++k) {\n",
        "            sum += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        // Store the sum in the corresponding element of matrix C\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to test the outer product implementation\n",
        "void testOuterProduct(float *A, float *B, float *C, int N) {\n",
        "    // Declare device pointers for matrices A, B, and C\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocate memory on the GPU for matrices A, B, and C\n",
        "    cudaMalloc((void **)&d_A, N * N * sizeof(float));\n",
        "    cudaMalloc((void **)&d_B, N * N * sizeof(float));\n",
        "    cudaMalloc((void **)&d_C, N * N * sizeof(float));\n",
        "\n",
        "    // Copy the data for matrices A and B from host to device\n",
        "    cudaMemcpy(d_A, A, N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define the number of threads per block and the number of blocks per grid\n",
        "    dim3 threadsPerBlock(2, 2);\n",
        "    dim3 blocksPerGrid(N / threadsPerBlock.x, N / threadsPerBlock.y);\n",
        "\n",
        "    // Launch the kernel\n",
        "    outerProductKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copy the resultant matrix C from device to host\n",
        "    cudaMemcpy(C, d_C, N * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free the allocated device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "}\n",
        "\n",
        "// Main function\n",
        "int main() {\n",
        "    // Define the dimension of the matrices\n",
        "    int N = 4;\n",
        "    // Initialize matrices A and B\n",
        "    float A[N * N] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};\n",
        "    float B[N * N] = {16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1};\n",
        "    // Declare the result matrix C\n",
        "    float C[N * N];\n",
        "\n",
        "    // Run the test function\n",
        "    testOuterProduct(A, B, C, N);\n",
        "\n",
        "    // Print the resultant matrix C\n",
        "    printf(\"Matrix C: \\n\");\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            printf(\"%f \", C[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Exit the program\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaHKBoMdmN28",
        "outputId": "1315ce58-5be1-4274-a34e-e874278172a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting outer_product_with_testbench.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the CUDA code with nvcc (NVIDIA CUDA Compiler)\n",
        "!nvcc outer_product_with_testbench.cu -o outer_product_with_testbench\n",
        "\n",
        "# Run the compiled CUDA program\n",
        "!./outer_product_with_testbench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRLQSOtYs953",
        "outputId": "8bc73b44-3f86-408f-dd77-fd6ddc75ab49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix C: \n",
            "80.000000 70.000000 60.000000 50.000000 \n",
            "240.000000 214.000000 188.000000 162.000000 \n",
            "400.000000 358.000000 316.000000 274.000000 \n",
            "560.000000 502.000000 444.000000 386.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Matrix Multiplication with Outer Product Approach\n",
        "\n",
        "This notebook demonstrates how to perform matrix multiplication using the outer product approach on a GPU using CUDA. The code consists of two major parts:\n",
        "\n",
        "### Kernel Function (`outerProductKernel`)\n",
        "\n",
        "The `outerProductKernel` function is the GPU kernel that performs the matrix multiplication in an outer-product-based approach. In this method, each thread is responsible for calculating an entire row-column multiplication, rather than a single element of the resultant matrix \\( C \\).\n",
        "\n",
        "- `__global__ void outerProductKernel(float *A, float *B, float *C, int N)`: This is the CUDA kernel function where the actual multiplication takes place.\n",
        "\n",
        "    - `float *A, *B, *C`: Pointers to matrices \\( A \\), \\( B \\), and \\( C \\) stored in the device memory.\n",
        "  \n",
        "    - `int N`: Dimension of the square matrices \\( A \\), \\( B \\), and \\( C \\).\n",
        "    \n",
        "    - `row` and `col`: Calculated to determine the row and column index for each thread.\n",
        "    \n",
        "    - `temp_C`: Variable to store the intermediate results of the outer product of two vectors for a specific element in \\( C \\).\n",
        "\n",
        "### Test Function (`testOuterProduct`)\n",
        "\n",
        "The `testOuterProduct` function serves as a testbench. It allocates device memory, copies the input matrices from host to device, launches the kernel, and then copies the resultant matrix back to the host.\n",
        "\n",
        "- `void testOuterProduct(float *A, float *B, float *C, int N)`: Function that prepares the data, calls the CUDA kernel, and retrieves the data.\n",
        "\n",
        "    - `float *d_A, *d_B, *d_C`: Device pointers for matrices \\( A \\), \\( B \\), and \\( C \\).\n",
        "  \n",
        "    - `cudaMalloc`: Allocates memory on the device.\n",
        "    \n",
        "    - `cudaMemcpy`: Transfers data between host and device.\n",
        "    \n",
        "    - `outerProductKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N)`: Kernel invocation.\n",
        "    \n",
        "    - `cudaFree`: Frees the allocated device memory.\n",
        "\n",
        "### Compilation and Execution\n",
        "\n",
        "The code is compiled using the NVIDIA CUDA Compiler (`nvcc`) and executed on the GPU. The resultant matrix \\( C \\) is then printed on the console.\n",
        "\n",
        "To run this code:\n",
        "\n",
        "1. Use the `%%writefile` magic command to write the CUDA code into a `.cu` file.\n",
        "2. Use `!nvcc` to compile the code.\n",
        "3. Run the compiled executable with `!./outer_product_with_testbench`.\n",
        "\n",
        "This approach leverages the parallel processing power of a GPU to execute the outer-product-based matrix multiplication efficiently."
      ],
      "metadata": {
        "id": "aVKhY7RrzMko"
      }
    }
  ]
}